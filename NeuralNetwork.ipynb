{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Implementation of fully connected neural network with a static architecture. \n",
    "\n",
    "- Input layer\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Output layer, using softmax as the activation function\n",
    "\n",
    "\n",
    "The model uses categorical crossentropy as its loss function. \n",
    "Optimized the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
    "The evaluation of the model is using accuracy.\n",
    "This was in an attempt to reproduce from scratch [example from the Keras documentation](https://keras.io/examples/mnist_mlp/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rPUmRqBtpS2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "     \n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "        \n",
    "    def __init__(self, epochs, learning_rate,X, Y,batch_size,iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.output = np.zeros((self.batch_size,self.Y.shape[1]))\n",
    "        self.dropout = 0.2\n",
    "        self.w1 = np.random.randn(self.X.shape[1],512) * np.sqrt(1/(512+784))\n",
    "        self.bias1 = np.random.randn(1,512)\n",
    "        self.a1 = np.random.randn(self.batch_size,512)\n",
    "        self.x_d1 = np.zeros((1,512)) #dropout vector \n",
    "        self.z1 = np.random.randn(self.batch_size,512)\n",
    "        self.vdw1 = 0\n",
    "        self.vdb1 = 0\n",
    "        \n",
    "        \n",
    "        self.w2 = np.random.randn(self.w1.shape[1],512) * np.sqrt(1/(512+self.w1.shape[1]))\n",
    "        self.bias2 = np.random.randn(1,512)\n",
    "        self.z2 =  np.random.randn(self.batch_size,512)\n",
    "        self.a2 = np.random.randn(self.batch_size,512)\n",
    "        self.x_d2 = np.zeros((1,512)) #dropout vector\n",
    "        self.vdw2 = 0\n",
    "        self.vdb2 = 0\n",
    "        \n",
    "        self.w3 = np.random.randn(self.w2.shape[1],10) * np.sqrt(1/(10+self.w2.shape[1]))\n",
    "        self.bias3 = np.random.randn(1,10)\n",
    "        self.z3 = np.random.randn(self.batch_size,10)\n",
    "        self.vdw3 = 0\n",
    "        self.vdb3 = 0\n",
    "        self.loss = []\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "                self.X = x_train[0: (1)*self.batch_size,:].reshape(self.batch_size,x_train.shape[1])\n",
    "                self.Y = y_train[0: (1)*self.batch_size,:].reshape(self.batch_size,y_train.shape[1])\n",
    "      \n",
    "                self.z1 = self.X.dot(self.w1) + self.bias1   \n",
    "                self.a1 = np.maximum(self.z1,0)\n",
    "\n",
    "                #dropout after layer 1\n",
    "                active_nodes = int((1-self.dropout)*(self.a1).shape[1])\n",
    "                active_node_list = sorted(random.sample(range(0,self.a1.shape[1]), active_nodes))\n",
    "                self.x_d1[:,active_node_list] = 1\n",
    "\n",
    "                self.z2 = ((self.a1* self.x_d1) @ (self.w2)) + self.bias2\n",
    "                self.a2 = np.maximum(self.z2,0)\n",
    "\n",
    "                #droput after layer2\n",
    "                active_nodes = int((1-self.dropout)*(self.a2).shape[1])\n",
    "                active_node_list = sorted(random.sample(range(0,(self.a2).shape[1]), active_nodes)) \n",
    "                self.x_d2[:,active_node_list] = 1 \n",
    "\n",
    "                self.z3 = ((self.a2*self.x_d2) @ self.w3) +  self.bias3\n",
    "                self.output = NeuralNetwork.softmax(self.z3)\n",
    "                loss = -np.sum(self.Y*np.log(self.output))/self.X.shape[0]\n",
    "                self.loss.append(loss)\n",
    "        for itr in range(1,self.iterations):\n",
    "            NeuralNetwork.backPropagation(self)\n",
    "            self.X = x_train[(itr*self.batch_size): (itr+1)*self.batch_size,:].reshape(self.batch_size,x_train.shape[1])\n",
    "            self.Y = y_train[(itr*self.batch_size): (itr+1)*self.batch_size,:].reshape(self.batch_size,y_train.shape[1])\n",
    "            NeuralNetwork.feedForward(self)\n",
    "\n",
    "    def feedForward(self):\n",
    "        self.z1 = self.X.dot(self.w1) + self.bias1\n",
    "        self.a1 = np.maximum(self.z1,0)\n",
    "       \n",
    "        #dropout after layer 1\n",
    "        active_nodes = int((1-self.dropout)*(self.a1).shape[1])\n",
    "        active_node_list = sorted(random.sample(range(0,self.a1.shape[1]), active_nodes))\n",
    "        self.x_d1[:,active_node_list] = 1\n",
    "        self.z2 = ((self.a1*self.x_d1) @ self.w2) + self.bias2\n",
    "        self.a2 = np.maximum(self.z2,0)\n",
    "        \n",
    "        #droput after layer2\n",
    "        active_nodes = int((1-self.dropout)*(self.a2).shape[1])\n",
    "        active_node_list = sorted(random.sample(range(0,(self.a2).shape[1]), active_nodes))\n",
    "        self.x_d2[:,active_node_list] = 1 \n",
    "        self.z3 = ((self.a2*self.x_d2) @ self.w3) +  self.bias3\n",
    "        self.output = NeuralNetwork.softmax(self.z3)\n",
    "\n",
    "    def predict(self,X):\n",
    "        z1 = X.dot(self.w1) + self.bias1\n",
    "        a1 = np.maximum(z1,0)\n",
    "        z2 = (a1 @ self.w2) + self.bias2\n",
    "        a2 = np.maximum(z2,0)\n",
    "        z3 = (a2 @ self.w3) +  self.bias3\n",
    "        return NeuralNetwork.softmax(z3)\n",
    "\n",
    "        \n",
    "    def backPropagation(self):\n",
    "        eps = 1e-8\n",
    "        de_do3 = self.output - self.Y     \n",
    "        din_w3 = self.a2\n",
    "        delta3 = (de_do3)\n",
    "        de_w3 = ((delta3.T @ din_w3).T)/self.X.shape[0]\n",
    "        de_b3 = (np.sum((de_do3),axis=0,keepdims=True))\n",
    "        self.vdw3 = self.vdw3 * 0.9 + (de_w3**2)*0.1\n",
    "        self.vdb3 = self.vdb3 * 0.9 + (de_b3**2)*0.1\n",
    "\n",
    "        de_do2 = delta3\n",
    "        do2_din =  NeuralNetwork.reluDerivative(self.z2)\n",
    "        din_w2 = self.a1\n",
    "        delta2 =  (self.w3 @ de_do2.T).T * (do2_din)\n",
    "        de_w2 =  (delta2.T @ din_w2)/self.X.shape[0]\n",
    "        de_b2 =  (np.sum((self.w3 @ (de_do2.T @ do2_din)),axis=0,keepdims=True))\n",
    "        self.vdw2 = self.vdw2 * 0.9 + (de_w2**2)*0.1\n",
    "        self.vdb2 = self.vdb2 * 0.9 + (de_b2**2)*0.1\n",
    "       \n",
    "        de_do1 = delta2 @ self.w2\n",
    "        do1_din =  NeuralNetwork.reluDerivative(self.z1)\n",
    "        din_w1 = self.X\n",
    "        de_w1 = (din_w1.T @ (do1_din * de_do1))/self.X.shape[0]\n",
    "        de_b1 = np.sum((de_do1.T @ do1_din),axis=0,keepdims=True) \n",
    "        #(np.ones((1,512)) @ (de_do1.T @ do1_din))\n",
    "        self.vdw1 = self.vdw1 * 0.9 + (de_w1**2)*0.1 \n",
    "        self.vdb1 = self.vdb1 * 0.9 + (de_b1**2)*0.1\n",
    "        \n",
    "#update w3       \n",
    "        self.w3 = self.w3 - self.learning_rate * de_w3/np.sqrt(self.vdw3+eps)\n",
    "        self.bias3 = self.bias3 - self.learning_rate * de_b3/np.sqrt(self.vdb3+eps)\n",
    "\n",
    "        \n",
    "#update w2\n",
    "        self.w2 = self.w2 - self.learning_rate * de_w2/ np.sqrt(self.vdw2+eps)\n",
    "        self.bias2 = self.bias2 - self.learning_rate * de_b2/ np.sqrt(self.vdb2+eps)\n",
    "\n",
    "    \n",
    "#update w1       \n",
    "        self.w1 = self.w1 - self.learning_rate * de_w1/np.sqrt(self.vdw1+eps)\n",
    "        self.bias1 = self.bias1 - self.learning_rate * de_b1/np.sqrt(self.vdb1+eps)\n",
    "\n",
    "       \n",
    "    \n",
    "    def reluDerivative(z):\n",
    "        z[z<=0] = 0\n",
    "        z[z>0] = 1\n",
    "        return z\n",
    "\n",
    "    def softmax(z):\n",
    "        expo = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return expo/expo.sum(axis=1, keepdims=True)\n",
    "     \n",
    "    def evaluate(Y_pred,Y_actual):\n",
    "        correct_predictions = 0\n",
    "        for i in range(Y_pred.shape[0]):\n",
    "            if np.argmax(np.asarray(Y_pred[i,:])) == np.argmax(np.asarray(Y_actual[i,:])):\n",
    "                correct_predictions += 1\n",
    "        return float(correct_predictions/Y_pred.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH3bgJyPuE2O"
   },
   "source": [
    "Train your fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation. Reporting accuracy on the folds, as well as on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsN4sUoUugl8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for fold  1   65.44821428571429\n",
      "Testing accuracy for fold  1   65.69285714285714\n",
      "Training accuracy for fold  2   69.28214285714286\n",
      "Testing accuracy for fold  2   68.88571428571429\n",
      "Training accuracy for fold  3   75.22142857142858\n",
      "Testing accuracy for fold  3   75.12857142857143\n",
      "Training accuracy for fold  4   65.89821428571429\n",
      "Testing accuracy for fold  4   65.69285714285714\n",
      "Training accuracy for fold  5   65.54642857142858\n",
      "Testing accuracy for fold  5   65.39285714285714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "X = np.concatenate((np.array(x_train),np.array(x_test)),axis = 0)\n",
    "Y = np.concatenate((np.array(y_train),np.array(y_test)),axis = 0)\n",
    "\n",
    "folds = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "i=0\n",
    "for train_index, test_index in folds.split(X):\n",
    "    i = i+1\n",
    "    X_train, X_test, Y_train, Y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "    n = NeuralNetwork(epochs=22, learning_rate=0.001,X=X_train, Y=Y_train,batch_size = 50 , iterations = 1120)\n",
    "    n.fit(X_train,Y_train)\n",
    "    Y_pred_train = n.predict(X_train)\n",
    "    Y_pred_test = n.predict(X_test)\n",
    "    print(\"Training accuracy for fold \",i,\" \",NeuralNetwork.evaluate(Y_pred_train,Y_train))\n",
    "    print(\"Testing accuracy for fold \",i, \" \",NeuralNetwork.evaluate(Y_pred_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links referred : \n",
    "https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
    "https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b\n",
    "\n",
    "\n",
    "Dropout code inspired from : \n",
    "https://www.python-course.eu/neural_networks_with_dropout.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A3-P556-F19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
